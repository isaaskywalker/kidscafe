import requests
from bs4 import BeautifulSoup
import datetime
import json
import os
import time
import random
import re
from sentiment import batch_analyze_reviews  # ê°ì • ë¶„ì„ ëª¨ë“ˆ import

def parse_korean_date(date_text):
    """í•œêµ­ì–´ ë‚ ì§œë¥¼ íŒŒì‹±í•˜ì—¬ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        date_text = date_text.strip()
        
        # "2025.06.15", "2025-06-15", "2025/06/15" í˜•ì‹
        if re.match(r'\d{4}[./-]\d{1,2}[./-]\d{1,2}', date_text):
            date_str = re.sub(r'[./-]', '-', date_text.split()[0])
            parts = date_str.split('-')
            if len(parts) == 3:
                year, month, day = parts
                return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        # "6ì›” 15ì¼", "2025ë…„ 6ì›” 15ì¼" í˜•ì‹
        if 'ë…„' in date_text and 'ì›”' in date_text:
            year_match = re.search(r'(\d{4})ë…„', date_text)
            month_match = re.search(r'(\d{1,2})ì›”', date_text)
            day_match = re.search(r'(\d{1,2})ì¼', date_text)
            
            year = year_match.group(1) if year_match else '2025'
            month = month_match.group(1).zfill(2) if month_match else '01'
            day = day_match.group(1).zfill(2) if day_match else '01'
            
            return f"{year}-{month}-{day}"
        
        return None
    except:
        return None

def is_recent_post(date_str, cutoff_date="2025-06-01"):
    """2025ë…„ 6ì›” ì´í›„ í¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸"""
    try:
        if not date_str:
            return False
        return date_str >= cutoff_date
    except:
        return False

def get_blog_post_date_and_content(link):
    """ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì˜ ë‚ ì§œì™€ ì½˜í…ì¸  ì¶”ì¶œ"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
        }
        
        resp = requests.get(link, headers=headers, timeout=15)
        if resp.status_code != 200:
            return None, None
            
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # ë‚ ì§œ ì¶”ì¶œ
        date_selectors = [
            '.se-module-text .se-fs- .se-ff-',
            '.blog_date', '.post-date', '.date',
            '[class*="date"]', '[class*="time"]',
            '.se_publishDate', '.se_publish_time'
        ]
        
        date = None
        for selector in date_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                parsed_date = parse_korean_date(text)
                if parsed_date:
                    date = parsed_date
                    break
            if date:
                break
        
        # ì½˜í…ì¸  ì¶”ì¶œ
        content_selectors = [
            '.se-main-container',
            '.se-component-wrap',
            '#postViewArea',
            '.post_content',
            '.blog_content',
            '[class*="content"]'
        ]
        
        content = None
        for selector in content_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                if text and len(text) > 200:
                    content = text
                    break
            if content:
                break
        
        return date, content
        
    except Exception as e:
        print(f"Error parsing {link}: {e}")
        return None, None

def crawl_naver_blog(keyword: str, max_page: int = 3):
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ (2025ë…„ 6ì›” ì´í›„ë§Œ)"""
    reviews = []
    
    for page in range(1, max_page + 1):
        print(f"ğŸ“„ Crawling page {page} for keyword: {keyword}")
        
        start = (page - 1) * 10 + 1
        url = f"https://search.naver.com/search.naver?where=post&sm=tab_jum&query={keyword}&start={start}"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": "https://www.naver.com"
        }
        
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                continue
                
            soup = BeautifulSoup(resp.text, 'html.parser')
            items = soup.select('.total_tit a.link_tit')
            
            print(f"ğŸ” Found {len(items)} items")
            
            for i, item in enumerate(items[:5]):
                title = item.get('title') or item.text.strip()
                link = item.get('href')
                
                if not link or not title or 'blog.naver.com' not in link:
                    continue
                
                print(f"ğŸ“ Processing: {title[:40]}...")
                
                # ë¸”ë¡œê·¸ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                date, content = get_blog_post_date_and_content(link)
                
                if not date or not content:
                    print("âŒ Failed to get date/content")
                    continue
                
                # 2025ë…„ 6ì›” ì´í›„ë§Œ í•„í„°ë§
                if not is_recent_post(date, "2025-06-01"):
                    print(f"âŒ Too old: {date}")
                    continue
                
                print(f"âœ… Recent post found: {date}")
                
                review = {
                    "title": title,
                    "link": link,
                    "date": date,
                    "content": content[:1000]  # ì²« 1000ì
                }
                
                reviews.append(review)
                print(f"âœ… Added review: {title[:30]}...")
                
                time.sleep(random.uniform(1, 2))
                
        except Exception as e:
            print(f"Error on page {page}: {e}")
        
        time.sleep(random.uniform(2, 4))
    
    return reviews

def crawl_naver_blog_multi(keywords, max_page=2):
    """ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§"""
    all_reviews = []
    seen_links = set()
    
    for keyword in keywords:
        print(f"\n=== ğŸ” Crawling: {keyword} ===")
        reviews = crawl_naver_blog(keyword, max_page)
        
        # ì¤‘ë³µ ì œê±°
        for review in reviews:
            if review['link'] not in seen_links:
                all_reviews.append(review)
                seen_links.add(review['link'])
    
    return all_reviews

def save_reviews_to_file(reviews, date_str):
    """ë¦¬ë·° ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    os.makedirs('data/reviews', exist_ok=True)
    path = f'data/reviews/{date_str}.json'
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(reviews, f, ensure_ascii=False, indent=2)
    print(f"âœ… Reviews saved to: {path}")
    return path

if __name__ == "__main__":
    print("ğŸš€ Starting crawler...")
    today = datetime.date.today().isoformat()
    
    keywords = [
        "ìš°ë¦¬ë¼ë¦¬ í‚¤ì¦ˆì¹´í˜ ëŒ€ì „ë¬¸í™”ì ",
        "ìš°ë¦¬ë¼ë¦¬ ëŒ€ì „ë¬¸í™”ì  ë¦¬ë·°"
    ]
    
    try:
        # 1. í¬ë¡¤ë§ ì‹¤í–‰
        reviews = crawl_naver_blog_multi(keywords, max_page=2)
        print(f"\nğŸ“Š ì´ {len(reviews)}ê°œì˜ ìµœì‹  ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ")
        
        if not reviews:
            print("âŒ ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            exit()
        
        # 2. ê°ì • ë¶„ì„ ì¶”ê°€
        print("ğŸ¤– ê°ì • ë¶„ì„ ì¤‘...")
        analyzed_reviews = batch_analyze_reviews(reviews)
        
        # 3. ê²°ê³¼ ì €ì¥
        save_path = save_reviews_to_file(analyzed_reviews, today)
        
        # 4. ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        print("\n" + "="*60)
        print("ğŸ“Š í¬ë¡¤ë§ & ë¶„ì„ ê²°ê³¼")
        print("="*60)
        
        for i, review in enumerate(analyzed_reviews[:3], 1):
            print(f"\n--- ë¦¬ë·° {i} ---")
            print(f"ì œëª©: {review['title']}")
            print(f"ë‚ ì§œ: {review['date']}")
            print(f"ê°ì •: {review['sentiment']} (ì‹ ë¢°ë„: {review['sentiment_confidence']})")
            print(f"ê·¼ê±°: {review['sentiment_reasoning']}")
            print(f"ë‚´ìš©: {review['content'][:100]}...")
        
        print(f"\nâœ… ì™„ë£Œ! ë°ì´í„° ì €ì¥ ìœ„ì¹˜: {save_path}")
        
    except Exception as e:
        print(f"âŒ ERROR: {e}")
        import traceback
        traceback.print_exc()
