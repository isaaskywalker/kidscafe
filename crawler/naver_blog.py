# iframe ì½˜í…ì¸  ì§ì ‘ ì ‘ê·¼ í¬ë¡¤ëŸ¬
import requests
from bs4 import BeautifulSoup
import datetime
import json
import os
import time
import random
import re
from urllib.parse import urlparse, parse_qs
from sentiment import batch_analyze_reviews

def extract_blog_info_from_url(blog_url):
    """ë¸”ë¡œê·¸ URLì—ì„œ blogIdì™€ logNo ì¶”ì¶œ"""
    try:
        # https://blog.naver.com/wlstjs20826/223886116495 í˜•ì‹
        parts = blog_url.split('/')
        if len(parts) >= 5:
            blog_id = parts[-2]
            log_no = parts[-1]
            return blog_id, log_no
        return None, None
    except:
        return None, None

def get_iframe_content_url(blog_id, log_no):
    """iframe ì½˜í…ì¸  URL ìƒì„±"""
    return f"https://blog.naver.com/PostView.naver?blogId={blog_id}&logNo={log_no}&redirect=Dlog&widgetTypeCall=true&noTrackingCode=true&directAccess=false"

def get_blog_post_date_and_content_iframe(link):
    """iframeì„ í†µí•´ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì˜ ë‚ ì§œì™€ ì½˜í…ì¸  ì¶”ì¶œ"""
    try:
        # 1. ì›ë³¸ URLì—ì„œ blogId, logNo ì¶”ì¶œ
        blog_id, log_no = extract_blog_info_from_url(link)
        if not blog_id or not log_no:
            print(f"âŒ Cannot extract blog info from {link}")
            return None, None
        
        # 2. iframe ì½˜í…ì¸  URL ìƒì„±
        iframe_url = get_iframe_content_url(blog_id, log_no)
        print(f"ğŸ”— Accessing iframe: {iframe_url}")
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Referer": link
        }
        
        resp = requests.get(iframe_url, headers=headers, timeout=15)
        if resp.status_code != 200:
            print(f"âŒ iframe access failed: {resp.status_code}")
            return None, None
            
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # 3. ë‚ ì§œ ì¶”ì¶œ (ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„)
        date_selectors = [
            '.se-publishDate',
            '.blog_date', 
            '.date',
            '[class*="date"]',
            '.se-module-text span',
            '.post_date',
            '.se_publishDate'
        ]
        
        date = None
        for selector in date_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text().strip()
                # ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
                date_match = re.search(r'(\d{4})[./-](\d{1,2})[./-](\d{1,2})', text)
                if date_match:
                    year, month, day = date_match.groups()
                    date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                    print(f"ğŸ“… Date found: {date} from {text}")
                    break
            if date:
                break
        
        # 4. ì½˜í…ì¸  ì¶”ì¶œ (ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìµœì‹  êµ¬ì¡°)
        content_selectors = [
            '.se-main-container',
            '.se-component-wrap',
            '.se-section-text',
            '.se-module-text',
            '#postViewArea',
            '.post_content',
            '.blog_content',
            '.se-text-paragraph'
        ]
        
        content = None
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                all_text = []
                for elem in elements:
                    text = elem.get_text().strip()
                    if text and len(text) > 10:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                        all_text.append(text)
                
                if all_text:
                    content = ' '.join(all_text)
                    print(f"ğŸ“ Content found: {len(content)} chars from {selector}")
                    break
        
        # 5. ë‚ ì§œê°€ ì—†ìœ¼ë©´ í˜„ì¬ ë‚ ì§œë¡œ ëŒ€ì²´ (ìµœì‹  ê²Œì‹œë¬¼ë¡œ ê°€ì •)
        if not date:
            date = "2025-06-10"
            print("ğŸ“… Using default date: 2025-06-10")
        
        # 6. ì½˜í…ì¸ ê°€ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
        if not content:
            all_text = soup.get_text()
            # ê¸´ ë¬¸ì¥ë“¤ ì¶”ì¶œ
            sentences = [s.strip() for s in all_text.split('.') if len(s.strip()) > 30]
            if sentences:
                content = '. '.join(sentences[:5])  # ì²˜ìŒ 5ë¬¸ì¥
                print(f"ğŸ“ Fallback content: {len(content)} chars")
        
        return date, content
        
    except Exception as e:
        print(f"âŒ Error parsing iframe {link}: {e}")
        return None, None

def crawl_naver_blog_with_iframe(keyword: str, max_page: int = 2):
    """iframeì„ í†µí•œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§"""
    reviews = []
    
    for page in range(1, max_page + 1):
        print(f"\nğŸ“„ Crawling page {page} for keyword: {keyword}")
        
        start = (page - 1) * 10 + 1
        url = f"https://search.naver.com/search.naver?where=post&sm=tab_jum&query={keyword}&start={start}"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": "https://www.naver.com"
        }
        
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            if resp.status_code != 200:
                continue
                
            soup = BeautifulSoup(resp.text, 'html.parser')
            items = soup.select('.total_tit a.link_tit')
            
            print(f"ğŸ” Found {len(items)} items")
            
            for i, item in enumerate(items[:3]):  # í˜ì´ì§€ë‹¹ 3ê°œë§Œ (ì•ˆì •ì„±)
                title = item.get('title') or item.text.strip()
                link = item.get('href')
                
                if not link or not title or 'blog.naver.com' not in link:
                    continue
                
                print(f"\nğŸ“ Processing: {title[:50]}...")
                print(f"ğŸ”— Link: {link}")
                
                # iframeì„ í†µí•œ ë‚ ì§œ/ì½˜í…ì¸  ì¶”ì¶œ
                date, content = get_blog_post_date_and_content_iframe(link)
                
                if not date or not content:
                    print("âŒ Failed to get date/content from iframe")
                    # ê¸°ë³¸ê°’ ì‚¬ìš©
                    date = "2025-06-10"
                    content = f"í‚¤ì¦ˆì¹´í˜ í›„ê¸°: {title}"
                
                # 2025ë…„ 6ì›” ì´í›„ë§Œ í•„í„°ë§
                if date < "2025-06-01":
                    print(f"âŒ Too old: {date}")
                    continue
                
                print(f"âœ… Recent post: {date}")
                
                review = {
                    "title": title,
                    "link": link,
                    "date": date,
                    "content": content[:1000]  # ìµœëŒ€ 1000ì
                }
                
                reviews.append(review)
                print(f"âœ… Added review: {title[:30]}...")
                
                # ìš”ì²­ ê°„ ë”œë ˆì´ (ì¤‘ìš”!)
                time.sleep(random.uniform(2, 4))
                
        except Exception as e:
            print(f"âŒ Error on page {page}: {e}")
        
        # í˜ì´ì§€ ê°„ ë”œë ˆì´
        time.sleep(random.uniform(3, 5))
    
    return reviews

def save_reviews_to_file(reviews, date_str):
    """ë¦¬ë·° ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
    os.makedirs('data/reviews', exist_ok=True)
    path = f'data/reviews/{date_str}_iframe.json'
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(reviews, f, ensure_ascii=False, indent=2)
    print(f"âœ… Reviews saved to: {path}")
    return path

if __name__ == "__main__":
    print("ğŸš€ Starting iframe-based crawler...")
    today = datetime.date.today().isoformat()
    
    keywords = [
        "ìš°ë¦¬ë¼ë¦¬ í‚¤ì¦ˆì¹´í˜ ëŒ€ì „ë¬¸í™”ì "
    ]
    
    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        all_reviews = []
        for keyword in keywords:
            print(f"\n{'='*60}")
            print(f"ğŸ” Crawling keyword: {keyword}")
            print('='*60)
            reviews = crawl_naver_blog_with_iframe(keyword, max_page=2)
            all_reviews.extend(reviews)
        
        print(f"\nğŸ“Š ì´ {len(all_reviews)}ê°œì˜ ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ")
        
        if not all_reviews:
            print("âŒ ìˆ˜ì§‘ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            exit()
        
        # ê°ì • ë¶„ì„ ì¶”ê°€
        print("\nğŸ¤– ê°ì • ë¶„ì„ ì¤‘...")
        analyzed_reviews = batch_analyze_reviews(all_reviews)
        
        # ê²°ê³¼ ì €ì¥
        save_path = save_reviews_to_file(analyzed_reviews, today)
        
        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        print("\n" + "="*60)
        print("ğŸ“Š iframe í¬ë¡¤ë§ ê²°ê³¼")
        print("="*60)
        
        for i, review in enumerate(analyzed_reviews, 1):
            print(f"\n--- ë¦¬ë·° {i} ---")
            print(f"ì œëª©: {review['title']}")
            print(f"ë‚ ì§œ: {review['date']}")
            print(f"ê°ì •: {review['sentiment']} (ì‹ ë¢°ë„: {review['sentiment_confidence']})")
            print(f"ê·¼ê±°: {review['sentiment_reasoning']}")
            print(f"ë§í¬: {review['link']}")
            print(f"ë‚´ìš©: {review['content'][:150]}...")
        
        print(f"\nâœ… ì™„ë£Œ! ë°ì´í„° ì €ì¥ ìœ„ì¹˜: {save_path}")
        
    except Exception as e:
        print(f"âŒ ERROR: {e}")
        import traceback
        traceback.print_exc()
